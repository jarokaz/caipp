{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m56",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "mp_kfp_custom_containers_resources.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWECL_E0S-cl"
      },
      "source": [
        "# Managed Pipelines Experimental: Custom containers and resource specs\n",
        "\n",
        "This notebook shows how to build and use custom containers for Pipeline components.  It also shows how to pass typed artifact data between component, and how to specify required resources when defining a pipeline.\n",
        "\n",
        "This example uses one of the TensorFlow Datasets, in particular the [Large Movie Review Dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewssubwords8k), for a binary sentiment classification task: predicting whether a movie review is negative or positive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNu_BtiA5h9N"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n",
        "\n",
        "This notebook is intended to be run on either one of:\n",
        "* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
        "* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
        "\n",
        "**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n",
        "\n",
        "We'll first install some libraries and set up some variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ-GWdI7SmrN"
      },
      "source": [
        "Set `gcloud` to use your project.  **Edit the following cell before running it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD5jOcSURdcU"
      },
      "source": [
        "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAaCPLjgiJrO"
      },
      "source": [
        "Set `gcloud` to use your project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWdxe4TXRHk"
      },
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gckGHdW9iPrq"
      },
      "source": [
        "If you're running this notebook on colab, authenticate with your user account:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQA0KrfXCvU"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaqJjbmk6o0o"
      },
      "source": [
        "-----------------\n",
        "\n",
        "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "**in the Terminal window** (which you can open via **File** > **New** in the menu).  You only need to do this once per notebook instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOpZ41iBW7bl"
      },
      "source": [
        "### Install the KFP SDK and AI Platform Pipelines client library\n",
        "\n",
        "For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "injJzlmllbEL"
      },
      "source": [
        "Then, install the libraries and restart the kernel. If you see a permissions error for the Metadata libraries, make sure you've run the `gcloud auth login` command as indicated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnRCttVlajjw"
      },
      "source": [
        "!gsutil cp gs://cloud-aiplatform-pipelines/releases/20210304/aiplatform_pipelines_client-0.1.0.caip20210304-py3-none-any.whl .\n",
        "# Get the Metadata SDK to query the produced metadata.\n",
        "!gsutil cp gs://cloud-aiplatform-metadata/sdk/google-cloud-aiplatform-metadata-0.0.1.tar.gz ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmUZzSv6YA9-"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  USER_FLAG = ''\n",
        "else:\n",
        "  USER_FLAG = '--user'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFSsfPr-Uad1"
      },
      "source": [
        "Install the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbZl0NsXSsmh"
      },
      "source": [
        "!python3 -m pip install {USER_FLAG} kfp==1.4 google-cloud-aiplatform-metadata-0.0.1.tar.gz aiplatform_pipelines_client-0.1.0.caip20210304-py3-none-any.whl --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5kaReN2lbEN"
      },
      "source": [
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N33S1ikHIOPS"
      },
      "source": [
        "The KFP version should be >= 1.4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4uvTyimMYOr"
      },
      "source": [
        "# Check the KFP version\n",
        "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1GX5KDOUJuI"
      },
      "source": [
        "If you're on colab, re-authorize after the kernel restart. **Edit the following cell for your project ID before running it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkxFp93xBk5"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n",
        "  !gcloud config set project {PROJECT_ID}\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  USER_FLAG = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tskC13YxW7b3"
      },
      "source": [
        "### Set some variables\n",
        "\n",
        "**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHsVifdTW7b4"
      },
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "# Required Parameters\n",
        "USER = 'YOUR_USER_NAME' # <---CHANGE THIS\n",
        "BUCKET_NAME = 'YOUR_BUCKET_NAME'  # <---CHANGE THIS\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
        "\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID'  # <---CHANGE THIS\n",
        "REGION = 'us-central1'\n",
        "API_KEY = 'YOUR_API_KEY'  # <---CHANGE THIS\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCi94vXkS-db"
      },
      "source": [
        "## Build custom container components\n",
        "\n",
        "\n",
        "We'll first build the two components that we'll use in our pipeline. The first component generates train and test data, and the second component consumes that data to train a model (to predict movie review sentiment).\n",
        "\n",
        "These components are based on custom Docker container images that we'll build and upload to the Google Container Registry, using Cloud Build."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBs5UldB4_jI"
      },
      "source": [
        "### Container 1: Generate examples\n",
        "\n",
        "First, we'll define and write out the `generate_examples.py` code.  It generates train and test set files from the [IMDB review data](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewssubwords8k), in `TFRecord` format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2J01k4hZaOm"
      },
      "source": [
        "!mkdir -p generate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48YXDQIiS-dc"
      },
      "source": [
        "%%writefile generate/generate_examples.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def _serialize_example(example, label):\n",
        "  example_value = tf.io.serialize_tensor(example).numpy()\n",
        "  label_value = tf.io.serialize_tensor(label).numpy()\n",
        "  feature = {\n",
        "      'examples':\n",
        "          tf.train.Feature(\n",
        "              bytes_list=tf.train.BytesList(value=[example_value])),\n",
        "      'labels':\n",
        "          tf.train.Feature(bytes_list=tf.train.BytesList(value=[label_value])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(\n",
        "      feature=feature)).SerializeToString()\n",
        "\n",
        "\n",
        "def _tf_serialize_example(example, label):\n",
        "  serialized_tensor = tf.py_function(_serialize_example, (example, label),\n",
        "                                     tf.string)\n",
        "  return tf.reshape(serialized_tensor, ())\n",
        "\n",
        "\n",
        "def generate_examples(training_data_uri, test_data_uri, config_file_uri):\n",
        "  (train_data, test_data), info = tfds.load(\n",
        "      # Use the version pre-encoded with an ~8k vocabulary.\n",
        "      'imdb_reviews/subwords8k',\n",
        "      # Return the train/test datasets as a tuple.\n",
        "      split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "      # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "      as_supervised=True,\n",
        "      with_info=True)\n",
        "\n",
        "  serialized_train_examples = train_data.map(_tf_serialize_example)\n",
        "  serialized_test_examples = test_data.map(_tf_serialize_example)\n",
        "\n",
        "  filename = os.path.join(training_data_uri, \"train.tfrecord\")\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(serialized_train_examples)\n",
        "\n",
        "  filename = os.path.join(test_data_uri, \"test.tfrecord\")\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(serialized_test_examples)\n",
        "\n",
        "  encoder = info.features['text'].encoder\n",
        "  config = {\n",
        "      'vocab_size': encoder.vocab_size,\n",
        "  }\n",
        "  config_file = os.path.join(config_file_uri, \"config\")\n",
        "  with tf.io.gfile.GFile(config_file, 'w') as f:\n",
        "    f.write(json.dumps(config))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--training_data_uri', type=str)\n",
        "  parser.add_argument('--test_data_uri', type=str)\n",
        "  parser.add_argument('--config_file_uri', type=str)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "  generate_examples(args.training_data_uri, args.test_data_uri,\n",
        "                    args.config_file_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRFMWnhPS-df"
      },
      "source": [
        "Next, we'll create a Dockerfile that builds a container to run `generate_examples.py`. We are using a Google [Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers) image as our base, since the image already includes most of what we need. \n",
        "You may use your own image as the base image instead. Note that we're also installing the `tensorflow_datasets` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzRx9XikS-df"
      },
      "source": [
        "%%writefile generate/Dockerfile\n",
        "\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
        "WORKDIR /pipeline\n",
        "COPY generate_examples.py generate_examples.py\n",
        "RUN pip install tensorflow_datasets\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMrMmGB5Hm8y"
      },
      "source": [
        "We'll use [Cloud Build](https://cloud.google.com/cloud-build/docs) to build the container image and write it to [GCR](https://cloud.google.com/container-registry)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ3Y9CErXs9M"
      },
      "source": [
        "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/custom-container-generate:{USER} generate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEJdna3RS-ds"
      },
      "source": [
        "### Container 2: Train Examples\n",
        "\n",
        "Next, we'll do the same for the 'Train Examples' custom container. We'll first write out a `train_examples.py` file, then build a container that runs it.  This script takes as input training and test data in `TFRecords` format and trains a Keras binary classification model to predict review sentiment. When training has finished, it writes out model and metrics information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he0VlF2Hbghh"
      },
      "source": [
        "!mkdir -p train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2t0eLglS-ds"
      },
      "source": [
        "%%writefile train/train_examples.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _parse_example(record):\n",
        "  f = {\n",
        "      'examples': tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
        "      'labels': tf.io.FixedLenFeature((), tf.string, default_value='')\n",
        "  }\n",
        "  return tf.io.parse_single_example(record, f)\n",
        "\n",
        "\n",
        "def _to_tensor(record):\n",
        "  examples = tf.io.parse_tensor(record['examples'], tf.int64)\n",
        "  labels = tf.io.parse_tensor(record['labels'], tf.int64)\n",
        "  return (examples, labels)\n",
        "\n",
        "\n",
        "def train_examples(training_data_uri, test_data_uri, config_file_uri,\n",
        "                   output_model_uri, output_metrics_uri):\n",
        "  train_examples = tf.data.TFRecordDataset(\n",
        "      [os.path.join(training_data_uri, 'train.tfrecord')])\n",
        "  test_examples = tf.data.TFRecordDataset(\n",
        "      [os.path.join(test_data_uri, 'test.tfrecord')])\n",
        "\n",
        "  train_batches = train_examples.map(_parse_example).map(_to_tensor)\n",
        "  test_batches = test_examples.map(_parse_example).map(_to_tensor)\n",
        "\n",
        "  with tf.io.gfile.GFile(os.path.join(config_file_uri, 'config')) as f:\n",
        "    config = json.loads(f.read())\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(config['vocab_size'], 16),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  train_batches = train_batches.shuffle(1000).padded_batch(\n",
        "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
        "\n",
        "  test_batches = test_batches.padded_batch(\n",
        "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
        "\n",
        "  history = model.fit(\n",
        "      train_batches,\n",
        "      epochs=10,\n",
        "      validation_data=test_batches,\n",
        "      validation_steps=30)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test_batches)\n",
        "\n",
        "  metrics = {\n",
        "      'loss': str(loss),\n",
        "      'accuracy': str(accuracy),\n",
        "  }\n",
        "\n",
        "  model_json = model.to_json()\n",
        "  with tf.io.gfile.GFile(os.path.join(output_model_uri, 'model.json'),\n",
        "                         'w') as f:\n",
        "    f.write(model_json)\n",
        "\n",
        "  with tf.io.gfile.GFile(os.path.join(output_metrics_uri, 'metrics.json'),\n",
        "                         'w') as f:\n",
        "    f.write(json.dumps(metrics))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--training_data_uri', type=str)\n",
        "  parser.add_argument('--test_data_uri', type=str)\n",
        "  parser.add_argument('--config_file_uri', type=str)\n",
        "  parser.add_argument('--output_model_uri', type=str)\n",
        "  parser.add_argument('--output_metrics_uri', type=str)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  train_examples(args.training_data_uri, args.test_data_uri,\n",
        "                 args.config_file_uri, args.output_model_uri,\n",
        "                 args.output_metrics_uri)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVJyxdeCS-du"
      },
      "source": [
        "Next, we'll create a Dockerfile that builds a container to run `train_examples.py`.  Again we're using a Google [Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers) image as our base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoDYRpzlS-dv"
      },
      "source": [
        "%%writefile train/Dockerfile\n",
        "\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
        "WORKDIR /pipeline\n",
        "COPY train_examples.py train_examples.py\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgd_zce_AXYv"
      },
      "source": [
        "We'll use [Cloud Build](https://cloud.google.com/cloud-build/docs) to build the container image and write it to [GCR](https://cloud.google.com/container-registry)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPmhXcXFbqVK"
      },
      "source": [
        "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/custom-container-train:{USER} train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3BqUxQ_9c31"
      },
      "source": [
        "### Create pipeline components using the custom container images\n",
        "\n",
        "Next, we'll define components for the 'generate' and 'train' steps, using the container images we just built.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2llvI0w9r3f"
      },
      "source": [
        "import time\n",
        "from kfp import components\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2 import compiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQAff2NRAHKZ"
      },
      "source": [
        "The 'generate' component specifies three outputs: training and test data, of type `Dataset`, and a config file, of type `File`. \n",
        "\n",
        "The component definition uses  `outputPath`  in specifying the `generate_example.py` script args.  These args are set to automatically-generated GCS URIs, and when `generate_examples` writes to those URIs, the outputs are available to downstream components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKqGpulg9b_J"
      },
      "source": [
        "generate_op = components.load_component_from_text(\"\"\"\n",
        "name: GenerateExamples\n",
        "outputs:\n",
        "- {name: training_data, type: Dataset}\n",
        "- {name: test_data, type: Dataset}\n",
        "- {name: config_file, type: File}\n",
        "implementation:\n",
        "  container:\n",
        "    image: gcr.io/%s/custom-container-generate:%s\n",
        "    command:\n",
        "    - python\n",
        "    - /pipeline/generate_examples.py\n",
        "    args:\n",
        "    - --training_data_uri\n",
        "    - {outputUri: training_data}\n",
        "    - --test_data_uri\n",
        "    - {outputUri: test_data}\n",
        "    - --config_file_uri\n",
        "    - {outputUri: config_file}\n",
        "\"\"\" % (PROJECT_ID, USER))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8qtwaqU9zbC"
      },
      "source": [
        "The train component takes as input training and test data of type `Dataset`, and a config `File`: it can consume the outputs of the \"generate\" component.   It specifies two outputs, one of type `Model` and one of type `Metrics`.\n",
        "\n",
        "The component definition uses  `inputPath` and `outputPath` when passing args to the `train_examples` script. So, the script's arg values will be GCS URIs, from which it will read its inputs and write its outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM_tmbv09prn"
      },
      "source": [
        "train_op = components.load_component_from_text(\"\"\"\n",
        "name: Train\n",
        "inputs:\n",
        "- {name: training_data, type: Dataset}\n",
        "- {name: test_data, type: Dataset}\n",
        "- {name: config_file, type: File}\n",
        "outputs:\n",
        "- {name: model, type: Model}\n",
        "- {name: metrics, type: Metrics}\n",
        "implementation:\n",
        "  container:\n",
        "    image: gcr.io/%s/custom-container-train:%s\n",
        "    command:\n",
        "    - python\n",
        "    - /pipeline/train_examples.py\n",
        "    args:\n",
        "    - --training_data_uri\n",
        "    - {inputUri: training_data}\n",
        "    - --test_data_uri\n",
        "    - {inputUri: test_data}\n",
        "    - --config_file_uri\n",
        "    - {inputUri: config_file}\n",
        "    - --output_model_uri\n",
        "    - {outputUri: model}\n",
        "    - --output_metrics_uri\n",
        "    - {outputUri: metrics}\n",
        "\"\"\" % (PROJECT_ID, USER))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1r1CWF0S-d7"
      },
      "source": [
        "## Define a KFP pipeline that uses the components\n",
        "\n",
        "Now we're ready to define a pipeline that uses these components. The `train` step takes its inputs from the `generate` step's outputs. \n",
        "\n",
        "Note also that we are able to define pipeline *resource* specs, which we do here for the training step, including memory constraints, the number of GPUs to allocate, and the type of accelerator to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYhopKUBS-d7"
      },
      "source": [
        "@dsl.pipeline(name='custom-container-pipeline-{}-{}'.format(USER, str(int(time.time()))))\n",
        "def pipeline():\n",
        "  generate = generate_op()\n",
        "  train = (train_op(\n",
        "      training_data=generate.outputs['training_data'],\n",
        "      test_data=generate.outputs['test_data'],\n",
        "      config_file=generate.outputs['config_file']).\n",
        "    set_cpu_limit('4').\n",
        "    set_memory_limit('14Gi').\n",
        "    add_node_selector_constraint(\n",
        "      'cloud.google.com/gke-accelerator',\n",
        "      'nvidia-tesla-k80').\n",
        "    set_gpu_limit(1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huP7loXFG6s8"
      },
      "source": [
        "Compile the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qf9KkkoA1y7"
      },
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline, \n",
        "                            pipeline_root=PIPELINE_ROOT,\n",
        "                            output_path='custom_container_pipeline_spec.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH5QFfSuW7cJ"
      },
      "source": [
        "### Submit the pipeline job\n",
        "\n",
        "Here, we'll create an API client using the API key you generated.\n",
        "\n",
        "Then, we'll submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that we're passing a `parameter_values` dict that specifies the pipeline input parameters we want to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnrYUDAW7cK"
      },
      "source": [
        "from aiplatform.pipelines import client\n",
        "\n",
        "api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n",
        "\n",
        "response = api_client.create_run_from_job_spec(\n",
        "    job_spec_path='custom_container_pipeline_spec.json',\n",
        "    name = 'my-pipeline-run-1'  # <- pipeline run name. Must be unique (change if you rerun)\n",
        "    # pipeline_root=PIPELINE_ROOT,  # optional- use if want to override compile-time value\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhfJYO3T613t"
      },
      "source": [
        "## Query the metadata produced by the pipeline.\n",
        "\n",
        "The set of artifacts and executions produced by the pipeline can also be queried using the AIPlatform Metadata SDK. The following shows a snippet for querying the metadata for a given pipeline run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZSsQHO1ZdQ"
      },
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "from google import auth\n",
        "from google.cloud.aiplatform_v1alpha1 import MetadataServiceClient\n",
        "from google.auth.transport import grpc, requests\n",
        "from google.cloud.aiplatform_v1alpha1.services.metadata_service.transports import grpc as transports_grpc\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _initialize_metadata_service_client() -> MetadataServiceClient:\n",
        "  scope = 'https://www.googleapis.com/auth/cloud-platform'\n",
        "  api_uri = 'us-central1-aiplatform.googleapis.com'\n",
        "  credentials, _ = auth.default(scopes=(scope,))\n",
        "  request = requests.Request()\n",
        "  channel = grpc.secure_authorized_channel(credentials, request, api_uri)\n",
        "\n",
        "  return MetadataServiceClient(\n",
        "      transport=transports_grpc.MetadataServiceGrpcTransport(channel=channel))\n",
        "\n",
        "client = _initialize_metadata_service_client()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBBZz5g41Ey"
      },
      "source": [
        "\n",
        "def get_run_context_name(pipeline_run):\n",
        "  contexts = client.list_contexts(parent='projects/{}/locations/{}/metadataStores/default'.format(PROJECT_ID, REGION))\n",
        "  for context in contexts:\n",
        "    if context.display_name == pipeline_run:\n",
        "      return context.name\n",
        "  \n",
        "run_context_name = get_run_context_name('my-pipeline-run-1')  # <- Name of the pipeline run\n",
        "\n",
        "client.query_context_lineage_subgraph(context=run_context_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kgtx8-bW7cM"
      },
      "source": [
        "### Monitor the pipeline run in the Cloud Console\n",
        "\n",
        "Once you've deployed the pipeline run, you can monitor it in the [Cloud Console](https://console.cloud.google.com/ai/platform/pipelines) under **AI Platform (Unified)** > **Pipelines**. \n",
        "\n",
        "Click in to the pipeline run to see the run graph (for our pipeline, this consists of two steps), and click on a step to view the job detail and the logs for that step.\n",
        "\n",
        "As you look at the pipeline graph, you'll see that you can inspect the artifacts passed between the pipeline steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFwR7J6O3TCq"
      },
      "source": [
        "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/generate_train.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/generate_train.png\" width=\"70%\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feV62LXyW7cN"
      },
      "source": [
        "## What next?\n",
        "\n",
        "Next, try out some of the other notebooks.\n",
        "\n",
        "- a [KFP intro notebook](https://colab.research.google.com/drive/1mrud9HjsVp5fToHwwNL0RotFtJCKtfZ1#scrollTo=feV62LXyW7cN).\n",
        "- a simple KFP example that [shows how data can be passed between pipeline steps](https://colab.research.google.com/drive/1NztsGV-FAp71MU7zfMHU0SlfQ8dpw-9u).\n",
        "\n",
        "- A TFX notebook that [shows the canonical 'Chicago taxi' example](https://colab.research.google.com/drive/1dNLlm21F6f5_4aeIg-Zs_F1iGGRPEvhW), and how to use custom Python functions and custom containers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89fYarRLW7cN"
      },
      "source": [
        "-----------------------------\n",
        "Copyright 2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    }
  ]
}